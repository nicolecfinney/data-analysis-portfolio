{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "df1 = pd.read_csv('/Users/nicolefinney/Dropbox/Mac/Desktop/sqlite/hello/hello/physical strength data/archive (3)/Sample_1.csv')\n",
    "df2 = pd.read_csv('/Users/nicolefinney/Dropbox/Mac/Desktop/sqlite/hello/hello/physical strength data/archive (3)/Sample_2.csv')\n",
    "df3 = pd.read_csv('/Users/nicolefinney/Dropbox/Mac/Desktop/sqlite/hello/hello/physical strength data/archive (3)/Sample_3.csv')\n",
    "df4 = pd.read_csv('/Users/nicolefinney/Dropbox/Mac/Desktop/sqlite/hello/hello/physical strength data/archive (3)/Sample_4.csv')\n",
    "df5 = pd.read_csv('/Users/nicolefinney/Dropbox/Mac/Desktop/sqlite/hello/hello/physical strength data/archive (3)/Sample_5_corrected.csv')\n",
    "\n",
    "df1.shape, df2.shape, df3.shape, df4.shape, df5.shape\n",
    "# ((175, 46), (209, 44), (370, 22), (340, 20), (305, 21))\n",
    "\n",
    "df1.columns,df2.columns,df3.columns,df3.columns,df5.columns\n",
    "\n",
    "## Creates two new dataframes - samples 1 and 2 are similar and samples 3, 4, and 5 are similar in columns\n",
    "df_1 = pd.concat([df1, df2], axis = 0, ignore_index=True)\n",
    "df_1.drop(['age','ethnicity'], inplace=True, axis=1)\n",
    "df_1['gender'] = df_1.female.map({0:'Male',1:'Female'})\n",
    "df_1.drop('female',inplace=True, axis=1)\n",
    "\n",
    "df_2 = pd.concat([df3, df4, df5], axis = 0, ignore_index=True)\n",
    "df_2.drop(['chest','ethnicity'], inplace=True, axis=1)\n",
    "df_2['gender'] = df_2.female.map({0:'Male',1:'Female'})\n",
    "df_2.drop('female',inplace=True, axis=1)\n",
    "\n",
    "## Finding null values\n",
    "df_1.isnull().sum()\n",
    "# 2 null values in chest strength - won't be using that so no big deal\n",
    "df_2.isnull().sum()\n",
    "# some null values in age - may or may not be an issue, come back later\n",
    "\n",
    "# Reverse coding of negatively keyed items: IP, scored out of 7 (df_1)\n",
    "# Items 6-10 for anxiety, fear, sentimentality\n",
    "# Anxiety:\n",
    "df_1['e_anx_6'] = df_1['e_anx_6'].map({1:7, 2:6, 3:5, 4:4, 5:3, 6:2, 7:1})\n",
    "df_1['e_anx_7'] = df_1['e_anx_7'].map({1:7, 2:6, 3:5, 4:4, 5:3, 6:2, 7:1})\n",
    "df_1['e_anx_8'] = df_1['e_anx_8'].map({1:7, 2:6, 3:5, 4:4, 5:3, 6:2, 7:1})\n",
    "df_1['e_anx_9'] = df_1['e_anx_9'].map({1:7, 2:6, 3:5, 4:4, 5:3, 6:2, 7:1})\n",
    "df_1['e_anx_10'] = df_1['e_anx_10'].map({1:7, 2:6, 3:5, 4:4, 5:3, 6:2, 7:1})\n",
    "\n",
    "#Fear:\n",
    "df_1['e_fear_6'] = df_1['e_fear_6'].map({1:7, 2:6, 3:5, 4:4, 5:3, 6:2, 7:1})\n",
    "df_1['e_fear_7'] = df_1['e_fear_7'].map({1:7, 2:6, 3:5, 4:4, 5:3, 6:2, 7:1})\n",
    "df_1['e_fear_8'] = df_1['e_fear_8'].map({1:7, 2:6, 3:5, 4:4, 5:3, 6:2, 7:1})\n",
    "df_1['e_fear_9'] = df_1['e_fear_9'].map({1:7, 2:6, 3:5, 4:4, 5:3, 6:2, 7:1})\n",
    "df_1['e_fear_10'] = df_1['e_fear_10'].map({1:7, 2:6, 3:5, 4:4, 5:3, 6:2, 7:1})\n",
    "\n",
    "#Sentimentality\n",
    "df_1['e_sen_6'] = df_1['e_sen_6'].map({1:7, 2:6, 3:5, 4:4, 5:3, 6:2, 7:1})\n",
    "df_1['e_sen_7'] = df_1['e_sen_7'].map({1:7, 2:6, 3:5, 4:4, 5:3, 6:2, 7:1})\n",
    "df_1['e_sen_8'] = df_1['e_sen_8'].map({1:7, 2:6, 3:5, 4:4, 5:3, 6:2, 7:1})\n",
    "df_1['e_sen_9'] = df_1['e_sen_9'].map({1:7, 2:6, 3:5, 4:4, 5:3, 6:2, 7:1})\n",
    "df_1['e_sen_10'] = df_1['e_sen_10'].map({1:7, 2:6, 3:5, 4:4, 5:3, 6:2, 7:1})\n",
    "\n",
    "#Reverse coding for hex items (df_2)\n",
    "# items 29, 35, 41, 59, 77, 89, and 95 need to be reversed\n",
    "df_2['hex_29'] = df_2['hex_29'].map({1:5, 2:4, 3:3, 4:2, 5:1})\n",
    "df_2['hex_35'] = df_2['hex_35'].map({1:5, 2:4, 3:3, 4:2, 5:1})\n",
    "df_2['hex_41'] = df_2['hex_41'].map({1:5, 2:4, 3:3, 4:2, 5:1})\n",
    "df_2['hex_59'] = df_2['hex_59'].map({1:5, 2:4, 3:3, 4:2, 5:1})\n",
    "df_2['hex_77'] = df_2['hex_77'].map({1:5, 2:4, 3:3, 4:2, 5:1})\n",
    "df_2['hex_89'] = df_2['hex_89'].map({1:5, 2:4, 3:3, 4:2, 5:1})\n",
    "df_2['hex_95'] = df_2['hex_95'].map({1:5, 2:4, 3:3, 4:2, 5:1})\n",
    "\n",
    "# Creating new total columns using reverse scores\n",
    "# df_1 (IP) scores\n",
    "df_1['anxiety'] =  df_1[['e_anx_1',\n",
    "                         'e_anx_2',\n",
    "                         'e_anx_3',\n",
    "                         'e_anx_4',\n",
    "                         'e_anx_5',\n",
    "                         'e_anx_6',\n",
    "                         'e_anx_7',\n",
    "                         'e_anx_8',\n",
    "                         'e_anx_9',\n",
    "                         'e_anx_10']].sum(axis=1)\n",
    "df_1['dependence'] =  df_1[['e_dep_1',\n",
    "                         'e_dep_2',\n",
    "                         'e_dep_3',\n",
    "                         'e_dep_4',\n",
    "                         'e_dep_5',\n",
    "                         'e_dep_6',\n",
    "                         'e_dep_7',\n",
    "                         'e_dep_8',\n",
    "                         'e_dep_9',\n",
    "                         'e_dep_10']].sum(axis=1)\n",
    "df_1['fearfulness'] =  df_1[['e_fear_1',\n",
    "                         'e_fear_2',\n",
    "                         'e_fear_3',\n",
    "                         'e_fear_4',\n",
    "                         'e_fear_5',\n",
    "                         'e_fear_6',\n",
    "                         'e_fear_7',\n",
    "                         'e_fear_8',\n",
    "                         'e_fear_9',\n",
    "                         'e_fear_10']].sum(axis=1)\n",
    "df_1['sentimentality'] =  df_1[['e_sen_1',\n",
    "                         'e_sen_2',\n",
    "                         'e_sen_3',\n",
    "                         'e_sen_4',\n",
    "                         'e_sen_5',\n",
    "                         'e_sen_6',\n",
    "                         'e_sen_7',\n",
    "                         'e_sen_8',\n",
    "                         'e_sen_9',\n",
    "                         'e_sen_10']].sum(axis=1)\n",
    "\n",
    "# Making sure we have our new columns for df_1\n",
    "df_1.head()\n",
    "\n",
    "#Creating total columns for our hex scores (df_2)\n",
    "df_2['fearfulness'] =  df_2[['hex_5',\n",
    "                             'hex_29',\n",
    "                             'hex_53',\n",
    "                             'hex_77']].sum(axis=1)\n",
    "df_2['anxiety'] =  df_2[['hex_11',\n",
    "                             'hex_35',\n",
    "                             'hex_59',\n",
    "                             'hex_83']].sum(axis=1)\n",
    "df_2['dependence'] =  df_2[['hex_17',\n",
    "                             'hex_41',\n",
    "                             'hex_65',\n",
    "                             'hex_89']].sum(axis=1)\n",
    "df_2['sentimentality'] =  df_2[['hex_23',\n",
    "                             'hex_47',\n",
    "                             'hex_71',\n",
    "                             'hex_95']].sum(axis=1)\n",
    "\n",
    "df_2.head()\n",
    "\n",
    "#Now that we have total columns for each of our variables, we can explore a bit.\n",
    "\n",
    "\n",
    "#Plotting the distributions for each of the traits we can see that it has a normal distribution.\n",
    "fig, axes = plt.subplots(2, 4, figsize = (40,20))\n",
    "sns.set(font_scale=1)\n",
    "traits = ['fearfulness', 'anxiety', 'dependence', 'sentimentality']\n",
    "\n",
    "counter = 0\n",
    "for trait in traits:\n",
    "    sns.countplot(x = trait,data = df_1, ax = axes[0, counter])\n",
    "    xmin, xmax = axes[0, counter].get_xlim()\n",
    "    custom_ticks = np.linspace(0, xmax, 12, dtype=int)\n",
    "    axes[0, counter].set_xticks(custom_ticks)\n",
    "    axes[0, counter].set_xticklabels(custom_ticks)\n",
    "    axes[0, counter].set_title('HEXICO-IPIP')\n",
    "    \n",
    "    \n",
    "    sns.countplot(x = trait, data = df_2, ax = axes[1,counter])\n",
    "    xmin, xmax = axes[1, counter].get_xlim()\n",
    "    custom_ticks = np.linspace(0, xmax, 18, dtype=int)\n",
    "    axes[1, counter].set_xticks(custom_ticks)\n",
    "    axes[1, counter].set_xticklabels(custom_ticks)\n",
    "    axes[1, counter].set_title('HEXICO-100')\n",
    "    counter +=1\n",
    "\n",
    "# Standardizing data\n",
    "def standardize(df, traits):\n",
    "    df_stand = df[traits]\n",
    "    df[traits] = (df_stand - df_stand.mean())/df_stand.std()\n",
    "    return df\n",
    "\n",
    "df_1 = standardize(df_1, traits)\n",
    "df_2 = standardize(df_2, traits)\n",
    "\n",
    "# Joining the two datasets\n",
    "df_1.grip = df_1.grip.astype('float')\n",
    "\n",
    "df_merged = pd.concat([df_1, df_2], join = 'inner')\n",
    "df_merged.head()\n",
    "\n",
    "## Do fearfulness and anxiety correlate with each other?\n",
    "sns.lmplot(data=df_merged, x='fearfulness', y='anxiety')\n",
    "from scipy import stats\n",
    "coefficient = stats.pearsonr(df_merged['fearfulness'], df_merged['anxiety'], alternative = 'two-sided')\n",
    "print(coefficient)\n",
    "np.corrcoef(df_merged['fearfulness'],df_merged['anxiety']) [0,1]\n",
    "    # very low p value - they are significantly correlated\n",
    "\n",
    "## Do dependence and anxiety correlate with each other?\n",
    "sns.lmplot(data=df_merged, x='dependence', y='anxiety')\n",
    "from scipy import stats\n",
    "coefficient = stats.pearsonr(df_merged['dependence'], df_merged['anxiety'], alternative = 'two-sided')\n",
    "print(coefficient)\n",
    "np.corrcoef(df_merged['dependence'],df_merged['anxiety']) [0,1]\n",
    "    # Dependence and anxiety are significantly correlated with each other\n",
    "\n",
    "## Do sentimentality and dependence correlate with each other?\n",
    "sns.lmplot(data=df_merged, x='sentimentality', y='dependence')\n",
    "from scipy import stats\n",
    "coefficient = stats.pearsonr(df_merged['sentimentality'], df_merged['dependence'], alternative = 'two-sided')\n",
    "print(coefficient)\n",
    "np.corrcoef(df_merged['sentimentality'],df_merged['dependence']) [0,1]\n",
    "    #Sentimentality and dependence are significantly correlated with one another\n",
    "\n",
    "#How do each of the four traits across both studies correlate with grip?\n",
    "fig, axes = plt.subplots(1, 4, figsize = (30,10))\n",
    "traits = ['fearfulness', 'anxiety', 'dependence', 'sentimentality']\n",
    "counter = 0\n",
    "for trait in traits:\n",
    "    sns.scatterplot(ax = axes[counter], x = trait, y = 'grip', hue = 'gender', data = df_merged)\n",
    "    counter += 1\n",
    "    print('Correlation between', trait, 'and grip is', df_merged[trait].corr(df_merged.grip))\n",
    "    print('Pearsonsr for', trait, 'and grip is', stats.pearsonr(df_merged[trait], df_merged.grip))\n",
    "## All 4 traits were found to be negatively correlated with grip with a significant p value."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
